{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Part 1"],"metadata":{"id":"11OG4mLKHA1h"}},{"cell_type":"code","execution_count":22,"metadata":{"id":"JrvjyLVf0M-4","executionInfo":{"status":"ok","timestamp":1769985885254,"user_tz":300,"elapsed":862,"user":{"displayName":"Darrick Ren","userId":"03492523889528788050"}},"outputId":"4082f292-342d-46e1-bafa-7f48cf020d85","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Number of missing values: 0\n","X shape: (584, 9)\n","y shape: (584,)\n"]}],"source":["# ====== 0) Mount Google Drive (Colab 里需要) ======\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# ====== 1) Set BASE_DIR to your Drive folder ======\n","import os\n","from pathlib import Path\n","\n","# 你的截图里文件在：MyDrive / \"linear regression AS\" / day.csv\n","BASE_DIR = Path(\"/content/drive/MyDrive/linear regression AS\")\n","\n","\n","day_csv = BASE_DIR / \"day.csv\"\n","\n","# ====== 2) Step 1: check missing, drop unwanted columns, shuffle ======\n","import pandas as pd\n","\n","data = pd.read_csv(day_csv)\n","\n","missing = data.isna().sum()\n","print(\"Number of missing values:\", int(missing.sum()))\n","\n","unwanted_columns = [\"instant\", \"dteday\", \"yr\", \"workingday\", \"casual\", \"registered\"]\n","data = data.drop(columns=[c for c in unwanted_columns if c in data.columns])\n","\n","data = data.sample(frac=1, random_state=38).reset_index(drop=True)\n","\n","step1_path = BASE_DIR / \"Step1_Checking&Dropping_unwanted_columns&Shuffling.csv\"\n","data.to_csv(step1_path, index=False)\n","\n","# ====== 3) Step 2: one-hot for selected columns, keep cnt as last column ======\n","data = pd.read_csv(step1_path)\n","\n","columns_need_splitting = [\"season\", \"mnth\", \"weekday\", \"weathersit\"]\n","data_with_One_hot = pd.get_dummies(\n","    data,\n","    columns=columns_need_splitting,\n","    drop_first=True,\n","    dtype=int\n",")\n","\n","# move \"cnt\" to the last column (same logic as you wrote)\n","cols = [c for c in data_with_One_hot.columns if c != \"cnt\"] + [\"cnt\"]\n","data_with_One_hot = data_with_One_hot[cols]\n","\n","step2_path = BASE_DIR / \"Step2_One_hot_state_splitting.csv\"\n","data_with_One_hot.to_csv(step2_path, index=False)\n","\n","# ====== 4) Step 3: 80/20 split, save 4 csv files ======\n","data_with = pd.read_csv(step2_path)\n","data_without = pd.read_csv(step1_path)\n","\n","n = len(data_with)\n","split = int(0.8 * n)\n","\n","with_train = data_with.iloc[:split].reset_index(drop=True)\n","with_test  = data_with.iloc[split:].reset_index(drop=True)\n","\n","without_train = data_without.iloc[:split].reset_index(drop=True)\n","without_test  = data_without.iloc[split:].reset_index(drop=True)\n","\n","with_train_path = BASE_DIR / \"With_One_hot_training_set.csv\"\n","with_test_path  = BASE_DIR / \"With_One_hot_testing_set.csv\"\n","without_train_path = BASE_DIR / \"Without_One_hot_training_set.csv\"\n","without_test_path  = BASE_DIR / \"Without_One_hot_testing_set.csv\"\n","\n","with_train.to_csv(with_train_path, index=False)\n","with_test.to_csv(with_test_path, index=False)\n","without_train.to_csv(without_train_path, index=False)\n","without_test.to_csv(without_test_path, index=False)\n","\n","# ====== 5) Load training set -> X, y (same as your code) ======\n","import numpy as np\n","\n","#train_path = with_train_path\n","train_path = without_train_path  # uncomment this if you train WITHOUT one-hot model\n","\n","data_train = pd.read_csv(train_path)\n","\n","X = data_train.iloc[:, :-1].to_numpy(dtype=float)\n","y = data_train.iloc[:, -1].to_numpy(dtype=float)\n","\n","print(\"X shape:\", X.shape)\n","print(\"y shape:\", y.shape)\n","\n","#Part two starts from here\n","\n","# I have loaded the matrix of features into 'X'\n","# The tags, the columns of cnt, is loaded into 'y'\n","\n","# X[i][1]  = holiday\n","# X[i][2]  = temp\n","# X[i][3]  = atemp\n","# X[i][4]  = hum\n","# X[i][5]  = windspeed\n","# All X[i][6-8] are 0 = season_1\n","# X[i][6]  = season_2\n","# X[i][7]  = season_3\n","# X[i][8]  = season_4\n","# All X[i][9-19] are 0 = mnth_1\n","# X[i][9]  = mnth_2\n","# X[i][10] = mnth_3\n","# X[i][11] = mnth_4\n","# X[i][12] = mnth_5\n","# X[i][13] = mnth_6\n","# X[i][14] = mnth_7\n","# X[i][15] = mnth_8\n","# X[i][16] = mnth_9\n","# X[i][17] = mnth_10\n","# X[i][18] = mnth_11\n","# X[i][19] = mnth_12\n","# All X[i][20-25] are 0 = weekday_0\n","# X[i][20] = weekday_1\n","# X[i][21] = weekday_2\n","# X[i][22] = weekday_3\n","# X[i][23] = weekday_4\n","# X[i][24] = weekday_5\n","# X[i][25] = weekday_6\n","# All X[i][26,27] are 0 = weathersit_1\n","# X[i][26] = weathersit_2\n","# X[i][27] = weathersit_3\n","# y[i] = cnt\n","\n","########### For without one hot training set:\n","# X[i][1] = season\n","# X[i][2] = mnth\n","# X[i][3] = holiday\n","# X[i][4] = weekday\n","# X[i][5] = weathersit\n","# X[i][6] = temp\n","# X[i][7] = atemp\n","# X[i][8] = hum\n","# X[i][9] = windspeed\n","# y[i]    = cnt"]},{"cell_type":"markdown","source":["Part 2"],"metadata":{"id":"dg94e9oAHEBo"}},{"cell_type":"code","source":["import numpy as np\n","\n","class LinearRegression:\n","    def __init__(self, add_bias=True):\n","        \"\"\"\n","        add_bias: if True, adds an intercept term to the model\n","        \"\"\"\n","        self.add_bias = add_bias\n","        self.w = None\n","\n","    def fit(self, x, y):\n","        \"\"\"\n","        x: (N,) or (N, D)\n","        y: (N,)\n","        \"\"\"\n","        x = np.asarray(x, dtype=float)\n","        y = np.asarray(y, dtype=float).reshape(-1)\n","\n","        # If x is 1D -> make it (N, 1)\n","        if x.ndim == 1:\n","            x = x[:, None]\n","\n","        N = x.shape[0]\n","\n","        # Add bias column (ones)\n","        if self.add_bias:\n","            x = np.column_stack([x, np.ones(N)])\n","\n","        # Numerically stable least squares (no explicit inverse)\n","        self.w = np.linalg.lstsq(x, y, rcond=None)[0]\n","        return self\n","\n","    def predict(self, x):\n","        \"\"\"\n","        x: (N,) or (N, D)\n","        return: (N,)\n","        \"\"\"\n","        if self.w is None:\n","            raise ValueError(\"Model not fitted. Call fit() first.\")\n","\n","        x = np.asarray(x, dtype=float)\n","\n","        if x.ndim == 1:\n","            x = x[:, None]\n","\n","        if self.add_bias:\n","            x = np.column_stack([x, np.ones(x.shape[0])])\n","\n","        return x @ self.w\n","\n","    @staticmethod\n","    def mse(y_true, y_pred):\n","        y_true = np.asarray(y_true, dtype=float).reshape(-1)\n","        y_pred = np.asarray(y_pred, dtype=float).reshape(-1)\n","        return float(np.mean((y_true - y_pred) ** 2))\n","\n","# ========== Load your train/test sets made by Part1 ==========\n","# Choose ONE of the two pipelines:\n","\n","# (A) With one-hot\n","#train_csv = BASE_DIR / \"With_One_hot_training_set.csv\"\n","#test_csv  = BASE_DIR / \"With_One_hot_testing_set.csv\"\n","\n","#(B) Without one-hot (uncomment to use)\n","train_csv = BASE_DIR / \"Without_One_hot_training_set.csv\"\n","test_csv  = BASE_DIR / \"Without_One_hot_testing_set.csv\"\n","\n","train_df = pd.read_csv(train_csv)\n","test_df  = pd.read_csv(test_csv)\n","\n","X_train = train_df.iloc[:, :-1].to_numpy(dtype=float)\n","y_train = train_df.iloc[:, -1].to_numpy(dtype=float)\n","\n","X_test  = test_df.iloc[:, :-1].to_numpy(dtype=float)\n","y_test  = test_df.iloc[:, -1].to_numpy(dtype=float)\n","\n","# ========== Train + Evaluate ==========\n","model = LinearRegression(add_bias=True)\n","model.fit(X_train, y_train)\n","\n","pred_train = model.predict(X_train)\n","pred_test  = model.predict(X_test)\n","\n","train_mse = model.mse(y_train, pred_train)\n","test_mse  = model.mse(y_test, pred_test)\n","\n","print(\"Train MSE:\", train_mse)\n","print(\"Test  MSE:\", test_mse)\n","\n","# Optional: show first few predictions\n","print(\"\\nFirst 5 predictions vs true:\")\n","for i in range(5):\n","    print(f\"pred={pred_test[i]:.3f}, true={y_test[i]:.3f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oPGc2Hz0EMOO","executionInfo":{"status":"ok","timestamp":1769985885294,"user_tz":300,"elapsed":31,"user":{"displayName":"Darrick Ren","userId":"03492523889528788050"}},"outputId":"c7fd9c4f-07a5-4d43-b977-ee8c0358e3b1"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Train MSE: 1783980.9127857122\n","Test  MSE: 1754347.2362024519\n","\n","First 5 predictions vs true:\n","pred=5538.161, true=7665.000\n","pred=3123.086, true=3624.000\n","pred=5144.847, true=4748.000\n","pred=4959.198, true=4186.000\n","pred=6295.611, true=4661.000\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Cvj3wfetK98f","executionInfo":{"status":"ok","timestamp":1769985885337,"user_tz":300,"elapsed":3,"user":{"displayName":"Darrick Ren","userId":"03492523889528788050"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["Non-linear part\n"],"metadata":{"id":"oSerz8VDRGxF"}},{"cell_type":"code","source":["import pandas as pd\n","# =========================================================\n","# FULL (simple prints, no fancy formatting, no plots)\n","# Methods:\n","#   0) Baseline\n","#   1) Polynomial (squares only)\n","#   2) Interaction (pairwise only)\n","#   3) Gaussian bases (D=10)\n","#   4) Sigmoid bases (D=5)\n","# =========================================================\n","\n","# ============ Load train/test ============\n","train_df = pd.read_csv(without_train_path)\n","test_df  = pd.read_csv(without_test_path)\n","\n","target_col = \"cnt\"\n","cont_cols = [\"temp\", \"atemp\", \"hum\", \"windspeed\"]\n","\n","# other features keep as-is (categorical / one-hot / other numeric)\n","other_cols = [c for c in train_df.columns if c not in cont_cols + [target_col]]\n","X_train_other = train_df[other_cols].to_numpy(dtype=float)\n","X_test_other  = test_df[other_cols].to_numpy(dtype=float)\n","\n","y_train = train_df[target_col].to_numpy(dtype=float)\n","y_test  = test_df[target_col].to_numpy(dtype=float)\n","\n","\n","# ============ 0) Baseline ============\n","X_train_base = train_df.drop(columns=[target_col]).to_numpy(dtype=float)\n","X_test_base  = test_df.drop(columns=[target_col]).to_numpy(dtype=float)\n","\n","base_model = LinearRegression(add_bias=True).fit(X_train_base, y_train)\n","pred_train_base = base_model.predict(X_train_base)\n","pred_test_base  = base_model.predict(X_test_base)\n","\n","base_train_mse = LinearRegression.mse(y_train, pred_train_base)\n","base_test_mse  = LinearRegression.mse(y_test, pred_test_base)\n","\n","print(\"===== Baseline =====\")\n","print(\"train mse:\", base_train_mse)\n","print(\"test  mse:\", base_test_mse)\n","\n","\n","# ============ 1) Polynomial (squares only) ============\n","train_poly = train_df.copy()\n","test_poly  = test_df.copy()\n","\n","for c in cont_cols:\n","    if c in train_poly.columns:\n","        train_poly[c + \"_sq\"] = train_poly[c] ** 2\n","        test_poly[c + \"_sq\"]  = test_poly[c] ** 2\n","\n","X_train_poly = train_poly.drop(columns=[target_col]).to_numpy(dtype=float)\n","X_test_poly  = test_poly.drop(columns=[target_col]).to_numpy(dtype=float)\n","\n","poly_model = LinearRegression(add_bias=True).fit(X_train_poly, y_train)\n","pred_train_poly = poly_model.predict(X_train_poly)\n","pred_test_poly  = poly_model.predict(X_test_poly)\n","\n","poly_train_mse = LinearRegression.mse(y_train, pred_train_poly)\n","poly_test_mse  = LinearRegression.mse(y_test, pred_test_poly)\n","\n","print(\"\\n===== Polynomial (squares only) =====\")\n","print(\"train mse:\", poly_train_mse)\n","print(\"test  mse:\", poly_test_mse)\n","\n","\n","# ============ 2) Interaction (pairwise only) ============\n","train_inter = train_df.copy()\n","test_inter  = test_df.copy()\n","\n","# all pairwise interactions among continuous features\n","for i in range(len(cont_cols)):\n","    for j in range(i + 1, len(cont_cols)):\n","        a, b = cont_cols[i], cont_cols[j]\n","        if a in train_inter.columns and b in train_inter.columns:\n","            train_inter[a + \"_x_\" + b] = train_inter[a] * train_inter[b]\n","            test_inter[a + \"_x_\" + b]  = test_inter[a] * test_inter[b]\n","\n","X_train_inter = train_inter.drop(columns=[target_col]).to_numpy(dtype=float)\n","X_test_inter  = test_inter.drop(columns=[target_col]).to_numpy(dtype=float)\n","\n","inter_model = LinearRegression(add_bias=True).fit(X_train_inter, y_train)\n","pred_train_inter = inter_model.predict(X_train_inter)\n","pred_test_inter  = inter_model.predict(X_test_inter)\n","\n","inter_train_mse = LinearRegression.mse(y_train, pred_train_inter)\n","inter_test_mse  = LinearRegression.mse(y_test, pred_test_inter)\n","\n","print(\"\\n===== Interaction (pairwise only) =====\")\n","print(\"train mse:\", inter_train_mse)\n","print(\"test  mse:\", inter_test_mse)\n","\n","\n","# ============ 3) Gaussian bases (D=10) ============\n","D_g = 10\n","gaussian = lambda x, mu, sigma: np.exp(-((x - mu) / sigma) ** 2)\n","\n","phi_train_list = []\n","phi_test_list  = []\n","\n","for col in cont_cols:\n","    xtr = train_df[col].to_numpy(dtype=float)\n","    xte = test_df[col].to_numpy(dtype=float)\n","\n","    mu = np.linspace(xtr.min(), xtr.max(), D_g)\n","    sigma = (mu[-1] - mu[0]) / max(D_g - 1, 1)\n","    if sigma <= 1e-8:\n","        sigma = 1e-2\n","\n","    phi_tr = gaussian(xtr[:, None], mu[None, :], sigma)  # (N_train, D_g)\n","    phi_te = gaussian(xte[:, None], mu[None, :], sigma)  # (N_test, D_g)\n","\n","    phi_train_list.append(phi_tr)\n","    phi_test_list.append(phi_te)\n","\n","Phi_train_g = np.hstack([X_train_other, np.hstack(phi_train_list)])\n","Phi_test_g  = np.hstack([X_test_other,  np.hstack(phi_test_list)])\n","\n","g_model = LinearRegression(add_bias=True).fit(Phi_train_g, y_train)\n","pred_train_g = g_model.predict(Phi_train_g)\n","pred_test_g  = g_model.predict(Phi_test_g)\n","\n","g_train_mse = LinearRegression.mse(y_train, pred_train_g)\n","g_test_mse  = LinearRegression.mse(y_test, pred_test_g)\n","\n","print(\"\\n===== Gaussian bases (D=10) =====\")\n","print(\"train mse:\", g_train_mse)\n","print(\"test  mse:\", g_test_mse)\n","\n","\n","# ============ 4) Sigmoid bases (D=5) ============\n","D_s = 5\n","sigmoid = lambda x, mu, s: 1 / (1 + np.exp(-(x - mu) / s))\n","\n","phi_train_list = []\n","phi_test_list  = []\n","\n","for col in cont_cols:\n","    xtr = train_df[col].to_numpy(dtype=float)\n","    xte = test_df[col].to_numpy(dtype=float)\n","\n","    mu = np.linspace(xtr.min(), xtr.max(), D_s)\n","    s = (mu[-1] - mu[0]) / max(D_s - 1, 1)\n","    if s <= 1e-8:\n","        s = 1e-2\n","\n","    phi_tr = sigmoid(xtr[:, None], mu[None, :], s)  # (N_train, D_s)\n","    phi_te = sigmoid(xte[:, None], mu[None, :], s)  # (N_test, D_s)\n","\n","    phi_train_list.append(phi_tr)\n","    phi_test_list.append(phi_te)\n","\n","Phi_train_s = np.hstack([X_train_other, np.hstack(phi_train_list)])\n","Phi_test_s  = np.hstack([X_test_other,  np.hstack(phi_test_list)])\n","\n","s_model = LinearRegression(add_bias=True).fit(Phi_train_s, y_train)\n","pred_train_s = s_model.predict(Phi_train_s)\n","pred_test_s  = s_model.predict(Phi_test_s)\n","\n","s_train_mse = LinearRegression.mse(y_train, pred_train_s)\n","s_test_mse  = LinearRegression.mse(y_test, pred_test_s)\n","\n","print(\"\\n===== Sigmoid bases (D=5) =====\")\n","print(\"train mse:\", s_train_mse)\n","print(\"test  mse:\", s_test_mse)\n","\n","\n","# ============ Simple summary ============\n","print(\"\\n===== Summary =====\")\n","print(\"Baseline train/test:\", base_train_mse, base_test_mse)\n","print(\"Poly     train/test:\", poly_train_mse, poly_test_mse)\n","print(\"Inter    train/test:\", inter_train_mse, inter_test_mse)\n","print(\"Gaussian train/test:\", g_train_mse, g_test_mse)\n","print(\"Sigmoid  train/test:\", s_train_mse, s_test_mse)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HKQPTjKtRLDR","executionInfo":{"status":"ok","timestamp":1769985955548,"user_tz":300,"elapsed":39,"user":{"displayName":"Darrick Ren","userId":"03492523889528788050"}},"outputId":"11f22c60-24d3-4a96-8f5f-e2c9d411a001"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["===== Baseline =====\n","train mse: 1783980.9127857122\n","test  mse: 1754347.2362024519\n","\n","===== Polynomial (squares only) =====\n","train mse: 1391124.2545031824\n","test  mse: 1474820.648210448\n","\n","===== Interaction (pairwise only) =====\n","train mse: 1484071.3749942277\n","test  mse: 1613529.0864299294\n","\n","===== Gaussian bases (D=10) =====\n","train mse: 1215247.7282587355\n","test  mse: 1493471.9879626255\n","\n","===== Sigmoid bases (D=5) =====\n","train mse: 1277469.9881892847\n","test  mse: 1426506.5941072472\n","\n","===== Summary =====\n","Baseline train/test: 1783980.9127857122 1754347.2362024519\n","Poly     train/test: 1391124.2545031824 1474820.648210448\n","Inter    train/test: 1484071.3749942277 1613529.0864299294\n","Gaussian train/test: 1215247.7282587355 1493471.9879626255\n","Sigmoid  train/test: 1277469.9881892847 1426506.5941072472\n"]}]}]}